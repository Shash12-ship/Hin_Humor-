# -*- coding: utf-8 -*-
"""word2vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uqQoTDEoqh9vF4mPV8epSq8YTpQeFm0J

Commented out IPython magic to ensure Python compatibility.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
from gensim.models import Word2Vec
from sklearn.decomposition import PCA
from matplotlib import pyplot
from matplotlib.font_manager import FontProperties
import os
from nltk import ngrams
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import GaussianNB
root = '/home/iiserb/Downloads'
# %matplotlib inline
import re
import warnings
import collections
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from gensim.models import Word2Vec
from sklearn.decomposition import PCA
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import normalize
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import f1_score
warnings.filterwarnings('ignore')

data = pd.DataFrame(pd.read_excel('Hin_humor.xlsx'))

for i in range(len(data)):
    data['Sentence'][i] = re.sub(r'[0-9a-zA-Z]','',data['Sentence'][i])

corpus = []
for x in data['Sentence']:
  corpus.append(x.split())

word_vec = Word2Vec(corpus, min_count=1, vector_size=100, epochs=500, alpha=0.01)

vocab_used = word_vec.wv.index_to_key

embeddings = dict.fromkeys(vocab_used)

pca = PCA(n_components=1)

full_embeddings = []

for key in embeddings.keys():
  full_embeddings.append(word_vec.wv.get_vector(key))

full_embeddings = np.array(full_embeddings)

full_embeddings = pca.fit_transform(full_embeddings)

for i,key in enumerate(embeddings.keys()):
  embeddings[key] = full_embeddings[i]

embeddings

X = []
for x in data['Sentence']:
  sample_row = []
  for key in embeddings.keys():
    if key in x:
      sample_row.append(embeddings[key][0])
    else:
      sample_row.append(0.0)
  X.append(sample_row)

pca = PCA(n_components=768)
X = pca.fit_transform(X)

X = np.array(X)

X.shape

Y = data['Tag']

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

def Logistic():
  grid = {'penalty': ['l1', 'l2'],
    'C': [0.1, 1, 10, 100] }
  model = GridSearchCV(estimator=LogisticRegression(),param_grid=grid,cv=10,scoring='f1')
  model.fit(X_train, Y_train)
  pred = model.predict(X_test)
  print('Best Paramters : ',model.best_params_)
  print(classification_report(Y_test, pred))
  print('Accuracy : ',accuracy_score(Y_test, pred))
  print(confusion_matrix(Y_test, pred))
  print('F1 Score For Humor : ',f1_score(Y_test,pred))

def SupportVectorMachines():
  grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}
  model = GridSearchCV(estimator=SVC(),param_grid=grid,cv=10,scoring='f1')
  model.fit(X_train, Y_train)
  pred = model.predict(X_test)
  print('Best Parameters : ',model.best_params_)
  print(classification_report(Y_test, pred))
  print('Accuracy : ',accuracy_score(Y_test, pred))
  print(confusion_matrix(Y_test, pred))
  print('F1 Score For Humor : ',f1_score(Y_test,pred))

def RandomForest():
  grid = {
    'bootstrap': [True],
    'max_depth': [80, 90, 100, 110],
    'max_features': [2, 3],
    'min_samples_leaf': [3, 4, 5],
    'min_samples_split': [8, 10, 12],
    'n_estimators': [100, 200, 300, 1000]
  }
  model = GridSearchCV(estimator=RandomForestClassifier(),param_grid=grid,cv=10,scoring='f1')
  model.fit(X_train, Y_train)
  pred = model.predict(X_test)
  print('Best Parameters : ',model.best_params_)
  print(classification_report(Y_test, pred))
  print('Accuracy : ',accuracy_score(Y_test, pred))
  print(confusion_matrix(Y_test, pred))
  print('F1 Score For Humor : ',f1_score(Y_test,pred))

def MultinomialNaB():
  grid = {'alpha': [0.01, 0.1, 0.5, 1.0, 10.0, ],
          'fit_prior': [True, False],
          'class_prior': [None, [0.1,]* 2, ]
         }
  model = GridSearchCV(estimator=MultinomialNB(),param_grid=grid,cv=10,scoring='f1')
  scaler = MinMaxScaler(feature_range=(0,52))
  X_tr = X_train
  X_te = X_test
  X_tr = scaler.fit_transform(X_tr)
  X_te = scaler.transform(X_te)
  model.fit(X_tr, Y_train)
  pred = model.predict(X_te)
  print('Best Parameters : ',model.best_params_)
  print(classification_report(Y_test, pred))
  print('Accuracy : ',accuracy_score(Y_test, pred))
  print(confusion_matrix(Y_test, pred))
  print('F1 Score For Humor : ',f1_score(Y_test,pred))

def GaussianNaB():
  grid = {'var_smoothing': np.logspace(0,-9, num=100)}
  model = GridSearchCV(estimator=GaussianNB(),param_grid=grid,cv=10,scoring='f1')
  model.fit(X_train, Y_train)
  pred = model.predict(X_test)
  print('Best Parameters : ',model.best_params_)
  print(classification_report(Y_test, pred))
  print('Accuracy : ',accuracy_score(Y_test, pred))
  print(confusion_matrix(Y_test, pred))
  print('F1 Score For Humor : ',f1_score(Y_test,pred))

print('\nLogistic Regression\n')
Logistic()
print('\n\n')

print('\nSupport Vector Machines\n')
SupportVectorMachines()
print('\n\n')

print('\nRandom Forest\n')
RandomForest()
print('\n\n')

print('\nMultinomial Naive Bayes\n')
MultinomialNaB()
print('\n\n')

print('\nGaussian Naive Bayes\n')
GaussianNaB()
print('\n\n')

