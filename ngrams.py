# -*- coding: utf-8 -*-
"""ngrams.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FZJ-RR8o0o8GSCz3wwNiGgUpOzlcs-ah
"""

# -*- coding: utf-8 -*-
"""ngrams.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FDsMCDz1Zs10-QgtxWz7VjfhCJy0BBur
"""

"""Commented out IPython magic to ensure Python compatibility."""

import numpy as np
import pandas as pd
from gensim.models import Word2Vec
from sklearn.decomposition import PCA
from matplotlib import pyplot
from matplotlib.font_manager import FontProperties
import os
from nltk import ngrams
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import GaussianNB
root = '/home/iiserb/Downloads'
# %matplotlib inline
import re
import warnings
import collections
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.model_selection import  GridSearchCV
from sklearn.metrics import f1_score
warnings.filterwarnings('ignore')

data = pd.DataFrame(pd.read_excel('Hin_humor.xlsx'))

for i in range(len(data)):
    data['Sentence'][i] = re.sub(r'[0-9a-zA-Z]','',data['Sentence'][i])

n_grams = []
for x in data['Sentence']:
    grams = ngrams(x.split(),3)
    n_grams.extend(grams)

print(n_grams[1])

gram_dict = dict.fromkeys(n_grams)

for x in n_grams:
    if gram_dict[x] == None:
        gram_dict[x] = 1
    else:
        gram_dict[x] += 1

gram_dict

keys = list(gram_dict.keys())
values = list(gram_dict.values())
sorted_value_index = np.argsort(values)
sorted_value_index = sorted_value_index[::-1]
sorted_dict = {keys[i]: values[i] for i in sorted_value_index}

sorted_dict

X = []
for x in data['Sentence']:
    grams = list(ngrams(x.split(),3))
    cnt = 2000
    sample_row = []
    for key in sorted_dict.keys():
        sample_row.append(grams.count(key))
        cnt -= 1
        if cnt < 1:
            break
    X.append(sample_row)

X = np.array(X)

Y = data['Tag']

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

def Logistic():
  grid = {'penalty': ['l1', 'l2'],
    'C': [0.1, 1, 10, 100] }
  model = GridSearchCV(estimator=LogisticRegression(),param_grid=grid,cv=10,scoring='f1')
  model.fit(X_train, Y_train)
  pred = model.predict(X_test)
  print('Best Paramters : ',model.best_params_)
  print(classification_report(Y_test, pred))
  print('Accuracy : ',accuracy_score(Y_test, pred))
  print(confusion_matrix(Y_test, pred))
  print('F1 Score For Humor : ',f1_score(Y_test,pred))

def SupportVectorMachines():
  grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}
  model = GridSearchCV(estimator=SVC(),param_grid=grid,cv=10,scoring='f1')
  model.fit(X_train, Y_train)
  pred = model.predict(X_test)
  print('Best Parameters : ',model.best_params_)
  print(classification_report(Y_test, pred))
  print('Accuracy : ',accuracy_score(Y_test, pred))
  print(confusion_matrix(Y_test, pred))
  print('F1 Score For Humor : ',f1_score(Y_test,pred))

def RandomForest():
  grid = {
    'bootstrap': [True],
    'max_depth': [80, 90, 100, 110],
    'max_features': [2, 3],
    'min_samples_leaf': [3, 4, 5],
    'min_samples_split': [8, 10, 12],
    'n_estimators': [100, 200, 300, 1000]
  }
  model = GridSearchCV(estimator=RandomForestClassifier(),param_grid=grid,cv=10,scoring='f1')
  model.fit(X_train, Y_train)
  pred = model.predict(X_test)
  print('Best Parameters : ',model.best_params_)
  print(classification_report(Y_test, pred))
  print('Accuracy : ',accuracy_score(Y_test, pred))
  print(confusion_matrix(Y_test, pred))
  print('F1 Score For Humor : ',f1_score(Y_test,pred))

def MultinomialNaB():
  grid = {'alpha': [0.01, 0.1, 0.5, 1.0, 10.0, ],
          'fit_prior': [True, False],
          'class_prior': [None, [0.1,]* 2, ]
         }
  model = GridSearchCV(estimator=MultinomialNB(),param_grid=grid,cv=10,scoring='f1')
  scaler = MinMaxScaler(feature_range=(0,52))
  X_tr = X_train
  X_te = X_test
  X_tr = scaler.fit_transform(X_tr)
  X_te = scaler.transform(X_te)
  model.fit(X_tr, Y_train)
  pred = model.predict(X_te)
  print('Best Parameters : ',model.best_params_)
  print(classification_report(Y_test, pred))
  print('Accuracy : ',accuracy_score(Y_test, pred))
  print(confusion_matrix(Y_test, pred))
  print('F1 Score For Humor : ',f1_score(Y_test,pred))

def GaussianNaB():
  grid = {'var_smoothing': np.logspace(0,-9, num=100)}
  model = GridSearchCV(estimator=GaussianNB(),param_grid=grid,cv=10,scoring='f1')
  model.fit(X_train, Y_train)
  pred = model.predict(X_test)
  print('Best Parameters : ',model.best_params_)
  print(classification_report(Y_test, pred))
  print('Accuracy : ',accuracy_score(Y_test, pred))
  print(confusion_matrix(Y_test, pred))
  print('F1 Score For Humor : ',f1_score(Y_test,pred))

print('\nLogistic Regression\n')
Logistic()
print('\n\n')

print('\nSupport Vector Machines\n')
SupportVectorMachines()
print('\n\n')

print('\nRandom Forest\n')
RandomForest()
print('\n\n')

print('\nMultinomial Naive Bayes\n')
MultinomialNaB()
print('\n\n')

print('\nGaussian Naive Bayes\n')
GaussianNaB()
print('\n\n')
